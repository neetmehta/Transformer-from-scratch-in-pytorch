{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d622909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_attention_weights(custom_attn, torch_attn):\n",
    "    # QKV weights: PyTorch uses one big linear layer for qkv\n",
    "    qkv_weight = torch_attn.self_attn.in_proj_weight\n",
    "    qkv_bias = torch_attn.self_attn.in_proj_bias\n",
    "    d_model = qkv_weight.shape[1]\n",
    "    d_k = d_model // custom_attn.h\n",
    "\n",
    "    # Split into Q, K, V\n",
    "    custom_attn.w_q.weight.data.copy_(qkv_weight[:d_model])\n",
    "    custom_attn.w_k.weight.data.copy_(qkv_weight[d_model:2*d_model])\n",
    "    custom_attn.w_v.weight.data.copy_(qkv_weight[2*d_model:])\n",
    "    \n",
    "    # Output projection\n",
    "    custom_attn.w_o.weight.data.copy_(torch_attn.self_attn.out_proj.weight)\n",
    "\n",
    "def copy_ffn_weights(custom_ffn, torch_ffn):\n",
    "    custom_ffn.linear_1.weight.data.copy_(torch_ffn.linear1.weight)\n",
    "    custom_ffn.linear_1.bias.data.copy_(torch_ffn.linear1.bias)\n",
    "    custom_ffn.linear_2.weight.data.copy_(torch_ffn.linear2.weight)\n",
    "    custom_ffn.linear_2.bias.data.copy_(torch_ffn.linear2.bias)\n",
    "\n",
    "def copy_layer_norm(custom_ln, torch_ln):\n",
    "    custom_ln.weight.data.copy_(torch_ln.weight)\n",
    "    custom_ln.bias.data.copy_(torch_ln.bias)\n",
    "\n",
    "def copy_encoder_weights(custom_encoder, torch_encoder):\n",
    "    for i, (c_layer, t_layer) in enumerate(zip(custom_encoder.encoder_layers, torch_encoder.layers)):\n",
    "        copy_attention_weights(c_layer.multi_headed_self_attention, t_layer)\n",
    "        copy_ffn_weights(c_layer.ffn, t_layer)  # Pass layer1 for dimension reference\n",
    "        copy_layer_norm(c_layer.norm1, t_layer.norm1)\n",
    "        copy_layer_norm(c_layer.norm2, t_layer.norm2)\n",
    "\n",
    "def copy_decoder_weights(custom_decoder, torch_decoder):\n",
    "    for i, (c_layer, t_layer) in enumerate(zip(custom_decoder.decoder_layers, torch_decoder.layers)):\n",
    "        # Masked self-attention\n",
    "        qkv_weight = t_layer.self_attn.in_proj_weight\n",
    "        d_model = qkv_weight.shape[1]\n",
    "        c_layer.masked_multi_headed_self_attention.w_q.weight.data.copy_(qkv_weight[:d_model])\n",
    "        c_layer.masked_multi_headed_self_attention.w_k.weight.data.copy_(qkv_weight[d_model:2*d_model])\n",
    "        c_layer.masked_multi_headed_self_attention.w_v.weight.data.copy_(qkv_weight[2*d_model:])\n",
    "        c_layer.masked_multi_headed_self_attention.w_o.weight.data.copy_(t_layer.self_attn.out_proj.weight)\n",
    "\n",
    "        # Cross-attention\n",
    "        cross_qkv_weight = t_layer.multihead_attn.in_proj_weight\n",
    "        c_layer.multi_headed_cross_attention.w_q.weight.data.copy_(cross_qkv_weight[:d_model])\n",
    "        c_layer.multi_headed_cross_attention.w_k.weight.data.copy_(cross_qkv_weight[d_model:2*d_model])\n",
    "        c_layer.multi_headed_cross_attention.w_v.weight.data.copy_(cross_qkv_weight[2*d_model:])\n",
    "        c_layer.multi_headed_cross_attention.w_o.weight.data.copy_(t_layer.multihead_attn.out_proj.weight)\n",
    "\n",
    "        # FFN and LayerNorm\n",
    "        copy_ffn_weights(c_layer.ffn, t_layer.linear1)\n",
    "        copy_layer_norm(c_layer.norm1, t_layer.norm1)\n",
    "        copy_layer_norm(c_layer.norm2, t_layer.norm2)\n",
    "        copy_layer_norm(c_layer.norm3, t_layer.norm3)\n",
    "\n",
    "def copy_transformer_weights(custom_transformer, torch_transformer):\n",
    "    copy_encoder_weights(custom_transformer.encoder, torch_transformer.encoder)\n",
    "    copy_decoder_weights(custom_transformer.decoder, torch_transformer.decoder)\n",
    "def copy_attention_weights(custom_attn, torch_attn):\n",
    "    # QKV weights: PyTorch uses one big linear layer for qkv\n",
    "    qkv_weight = torch_attn.self_attn.in_proj_weight\n",
    "    qkv_bias = torch_attn.self_attn.in_proj_bias\n",
    "    d_model = qkv_weight.shape[1]\n",
    "    d_k = d_model // custom_attn.h\n",
    "\n",
    "    # Split into Q, K, V\n",
    "    custom_attn.w_q.weight.data.copy_(qkv_weight[:d_model])\n",
    "    custom_attn.w_k.weight.data.copy_(qkv_weight[d_model:2*d_model])\n",
    "    custom_attn.w_v.weight.data.copy_(qkv_weight[2*d_model:])\n",
    "    \n",
    "    # Output projection\n",
    "    custom_attn.w_o.weight.data.copy_(torch_attn.self_attn.out_proj.weight)\n",
    "\n",
    "def copy_ffn_weights(custom_ffn, torch_ffn):\n",
    "    custom_ffn.linear_1.weight.data.copy_(torch_ffn.linear1.weight)\n",
    "    custom_ffn.linear_1.bias.data.copy_(torch_ffn.linear1.bias)\n",
    "    custom_ffn.linear_2.weight.data.copy_(torch_ffn.linear2.weight)\n",
    "    custom_ffn.linear_2.bias.data.copy_(torch_ffn.linear2.bias)\n",
    "\n",
    "def copy_layer_norm(custom_ln, torch_ln):\n",
    "    custom_ln.weight.data.copy_(torch_ln.weight)\n",
    "    custom_ln.bias.data.copy_(torch_ln.bias)\n",
    "\n",
    "def copy_encoder_weights(custom_encoder, torch_encoder):\n",
    "    for i, (c_layer, t_layer) in enumerate(zip(custom_encoder.encoder_layers, torch_encoder.layers)):\n",
    "        copy_attention_weights(c_layer.multi_headed_self_attention, t_layer)\n",
    "        copy_ffn_weights(c_layer.ffn, t_layer)  # Pass layer1 for dimension reference\n",
    "        copy_layer_norm(c_layer.norm1, t_layer.norm1)\n",
    "        copy_layer_norm(c_layer.norm2, t_layer.norm2)\n",
    "\n",
    "def copy_decoder_weights(custom_decoder, torch_decoder):\n",
    "    for i, (c_layer, t_layer) in enumerate(zip(custom_decoder.decoder_layers, torch_decoder.layers)):\n",
    "        # Masked self-attention\n",
    "        qkv_weight = t_layer.self_attn.in_proj_weight\n",
    "        d_model = qkv_weight.shape[1]\n",
    "        c_layer.masked_multi_headed_self_attention.w_q.weight.data.copy_(qkv_weight[:d_model])\n",
    "        c_layer.masked_multi_headed_self_attention.w_k.weight.data.copy_(qkv_weight[d_model:2*d_model])\n",
    "        c_layer.masked_multi_headed_self_attention.w_v.weight.data.copy_(qkv_weight[2*d_model:])\n",
    "        c_layer.masked_multi_headed_self_attention.w_o.weight.data.copy_(t_layer.self_attn.out_proj.weight)\n",
    "\n",
    "        # Cross-attention\n",
    "        cross_qkv_weight = t_layer.multihead_attn.in_proj_weight\n",
    "        c_layer.multi_headed_cross_attention.w_q.weight.data.copy_(cross_qkv_weight[:d_model])\n",
    "        c_layer.multi_headed_cross_attention.w_k.weight.data.copy_(cross_qkv_weight[d_model:2*d_model])\n",
    "        c_layer.multi_headed_cross_attention.w_v.weight.data.copy_(cross_qkv_weight[2*d_model:])\n",
    "        c_layer.multi_headed_cross_attention.w_o.weight.data.copy_(t_layer.multihead_attn.out_proj.weight)\n",
    "\n",
    "        # FFN and LayerNorm\n",
    "        copy_ffn_weights(c_layer.ffn, t_layer)\n",
    "        copy_layer_norm(c_layer.norm1, t_layer.norm1)\n",
    "        copy_layer_norm(c_layer.norm2, t_layer.norm2)\n",
    "        copy_layer_norm(c_layer.norm3, t_layer.norm3)\n",
    "\n",
    "def copy_transformer_weights(custom_transformer, torch_transformer):\n",
    "    copy_encoder_weights(custom_transformer.encoder, torch_transformer.encoder)\n",
    "    copy_decoder_weights(custom_transformer.decoder, torch_transformer.decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920cffe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n",
      "Reusing dataset wmt14 (C:\\Users\\neetm\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\3d7d25048da28a2f2a8dda5ca306bdc4affcf02bbcb3d277cfe2d5d7b1d71ebc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835a52cfe3634bc39cb0f8eb65ebe996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!cd /content/Transformer-from-scratch-in-pytorch\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/content/Transformer-from-scratch-in-pytorch')\n",
    "os.environ['PYTHONPATH'] = \"/content/Transformer-from-scratch-in-pytorch\"\n",
    "from configs.transformer_tiny import Config\n",
    "from tokenizer import TokenizerWrapper\n",
    "from data import WMTENDE, CustomBatchSampler, pad_collate_fn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from model import build_transformer\n",
    "\n",
    "def copy_attention_weights(custom_attn, torch_attn):\n",
    "    # QKV weights: PyTorch uses one big linear layer for qkv\n",
    "    qkv_weight = torch_attn.self_attn.in_proj_weight\n",
    "    qkv_bias = torch_attn.self_attn.in_proj_bias\n",
    "    d_model = qkv_weight.shape[1]\n",
    "    d_k = d_model // custom_attn.h\n",
    "\n",
    "    # Split into Q, K, V\n",
    "    custom_attn.w_q.weight.data.copy_(qkv_weight[:d_model])\n",
    "    custom_attn.w_k.weight.data.copy_(qkv_weight[d_model:2*d_model])\n",
    "    custom_attn.w_v.weight.data.copy_(qkv_weight[2*d_model:])\n",
    "\n",
    "    # Output projection\n",
    "    custom_attn.w_o.weight.data.copy_(torch_attn.self_attn.out_proj.weight)\n",
    "\n",
    "def copy_ffn_weights(custom_ffn, torch_ffn):\n",
    "    custom_ffn.linear_1.weight.data.copy_(torch_ffn.linear1.weight)\n",
    "    custom_ffn.linear_1.bias.data.copy_(torch_ffn.linear1.bias)\n",
    "    custom_ffn.linear_2.weight.data.copy_(torch_ffn.linear2.weight)\n",
    "    custom_ffn.linear_2.bias.data.copy_(torch_ffn.linear2.bias)\n",
    "\n",
    "def copy_layer_norm(custom_ln, torch_ln):\n",
    "    custom_ln.weight.data.copy_(torch_ln.weight)\n",
    "    custom_ln.bias.data.copy_(torch_ln.bias)\n",
    "\n",
    "def copy_encoder_weights(custom_encoder, torch_encoder):\n",
    "    for i, (c_layer, t_layer) in enumerate(zip(custom_encoder.encoder_layers, torch_encoder.layers)):\n",
    "        copy_attention_weights(c_layer.multi_headed_self_attention, t_layer)\n",
    "        copy_ffn_weights(c_layer.ffn, t_layer)  # Pass layer1 for dimension reference\n",
    "        copy_layer_norm(c_layer.norm1, t_layer.norm1)\n",
    "        copy_layer_norm(c_layer.norm2, t_layer.norm2)\n",
    "\n",
    "def copy_decoder_weights(custom_decoder, torch_decoder):\n",
    "    for i, (c_layer, t_layer) in enumerate(zip(custom_decoder.decoder_layers, torch_decoder.layers)):\n",
    "        # Masked self-attention\n",
    "        qkv_weight = t_layer.self_attn.in_proj_weight\n",
    "        d_model = qkv_weight.shape[1]\n",
    "        c_layer.masked_multi_headed_self_attention.w_q.weight.data.copy_(qkv_weight[:d_model])\n",
    "        c_layer.masked_multi_headed_self_attention.w_k.weight.data.copy_(qkv_weight[d_model:2*d_model])\n",
    "        c_layer.masked_multi_headed_self_attention.w_v.weight.data.copy_(qkv_weight[2*d_model:])\n",
    "        c_layer.masked_multi_headed_self_attention.w_o.weight.data.copy_(t_layer.self_attn.out_proj.weight)\n",
    "\n",
    "        # Cross-attention\n",
    "        cross_qkv_weight = t_layer.multihead_attn.in_proj_weight\n",
    "        c_layer.multi_headed_cross_attention.w_q.weight.data.copy_(cross_qkv_weight[:d_model])\n",
    "        c_layer.multi_headed_cross_attention.w_k.weight.data.copy_(cross_qkv_weight[d_model:2*d_model])\n",
    "        c_layer.multi_headed_cross_attention.w_v.weight.data.copy_(cross_qkv_weight[2*d_model:])\n",
    "        c_layer.multi_headed_cross_attention.w_o.weight.data.copy_(t_layer.multihead_attn.out_proj.weight)\n",
    "\n",
    "        # FFN and LayerNorm\n",
    "        copy_ffn_weights(c_layer.ffn, t_layer.linear1)\n",
    "        copy_layer_norm(c_layer.norm1, t_layer.norm1)\n",
    "        copy_layer_norm(c_layer.norm2, t_layer.norm2)\n",
    "        copy_layer_norm(c_layer.norm3, t_layer.norm3)\n",
    "\n",
    "def copy_transformer_weights(custom_transformer, torch_transformer):\n",
    "    copy_encoder_weights(custom_transformer.encoder, torch_transformer.encoder)\n",
    "    copy_decoder_weights(custom_transformer.decoder, torch_transformer.decoder)\n",
    "def copy_attention_weights(custom_attn, torch_attn):\n",
    "    # QKV weights: PyTorch uses one big linear layer for qkv\n",
    "    qkv_weight = torch_attn.self_attn.in_proj_weight\n",
    "    qkv_bias = torch_attn.self_attn.in_proj_bias\n",
    "    d_model = qkv_weight.shape[1]\n",
    "    d_k = d_model // custom_attn.h\n",
    "\n",
    "    # Split into Q, K, V\n",
    "    custom_attn.w_q.weight.data.copy_(qkv_weight[:d_model])\n",
    "    custom_attn.w_k.weight.data.copy_(qkv_weight[d_model:2*d_model])\n",
    "    custom_attn.w_v.weight.data.copy_(qkv_weight[2*d_model:])\n",
    "\n",
    "    # Output projection\n",
    "    custom_attn.w_o.weight.data.copy_(torch_attn.self_attn.out_proj.weight)\n",
    "\n",
    "def copy_ffn_weights(custom_ffn, torch_ffn):\n",
    "    custom_ffn.linear_1.weight.data.copy_(torch_ffn.linear1.weight)\n",
    "    custom_ffn.linear_1.bias.data.copy_(torch_ffn.linear1.bias)\n",
    "    custom_ffn.linear_2.weight.data.copy_(torch_ffn.linear2.weight)\n",
    "    custom_ffn.linear_2.bias.data.copy_(torch_ffn.linear2.bias)\n",
    "\n",
    "def copy_layer_norm(custom_ln, torch_ln):\n",
    "    custom_ln.weight.data.copy_(torch_ln.weight)\n",
    "    custom_ln.bias.data.copy_(torch_ln.bias)\n",
    "\n",
    "def copy_encoder_weights(custom_encoder, torch_encoder):\n",
    "    for i, (c_layer, t_layer) in enumerate(zip(custom_encoder.encoder_layers, torch_encoder.layers)):\n",
    "        copy_attention_weights(c_layer.multi_headed_self_attention, t_layer)\n",
    "        copy_ffn_weights(c_layer.ffn, t_layer)  # Pass layer1 for dimension reference\n",
    "        copy_layer_norm(c_layer.norm1, t_layer.norm1)\n",
    "        copy_layer_norm(c_layer.norm2, t_layer.norm2)\n",
    "\n",
    "def copy_decoder_weights(custom_decoder, torch_decoder):\n",
    "    for i, (c_layer, t_layer) in enumerate(zip(custom_decoder.decoder_layers, torch_decoder.layers)):\n",
    "        # Masked self-attention\n",
    "        qkv_weight = t_layer.self_attn.in_proj_weight\n",
    "        d_model = qkv_weight.shape[1]\n",
    "        c_layer.masked_multi_headed_self_attention.w_q.weight.data.copy_(qkv_weight[:d_model])\n",
    "        c_layer.masked_multi_headed_self_attention.w_k.weight.data.copy_(qkv_weight[d_model:2*d_model])\n",
    "        c_layer.masked_multi_headed_self_attention.w_v.weight.data.copy_(qkv_weight[2*d_model:])\n",
    "        c_layer.masked_multi_headed_self_attention.w_o.weight.data.copy_(t_layer.self_attn.out_proj.weight)\n",
    "\n",
    "        # Cross-attention\n",
    "        cross_qkv_weight = t_layer.multihead_attn.in_proj_weight\n",
    "        c_layer.multi_headed_cross_attention.w_q.weight.data.copy_(cross_qkv_weight[:d_model])\n",
    "        c_layer.multi_headed_cross_attention.w_k.weight.data.copy_(cross_qkv_weight[d_model:2*d_model])\n",
    "        c_layer.multi_headed_cross_attention.w_v.weight.data.copy_(cross_qkv_weight[2*d_model:])\n",
    "        c_layer.multi_headed_cross_attention.w_o.weight.data.copy_(t_layer.multihead_attn.out_proj.weight)\n",
    "\n",
    "        # FFN and LayerNorm\n",
    "        copy_ffn_weights(c_layer.ffn, t_layer)\n",
    "        copy_layer_norm(c_layer.norm1, t_layer.norm1)\n",
    "        copy_layer_norm(c_layer.norm2, t_layer.norm2)\n",
    "        copy_layer_norm(c_layer.norm3, t_layer.norm3)\n",
    "\n",
    "def copy_transformer_weights(custom_transformer, torch_transformer):\n",
    "    copy_encoder_weights(custom_transformer.encoder, torch_transformer.encoder)\n",
    "    copy_decoder_weights(custom_transformer.decoder, torch_transformer.decoder)\n",
    "\n",
    "config = Config()\n",
    "tokenizer = TokenizerWrapper(config)\n",
    "data = WMTENDE(config, tokenizer, 'test')\n",
    "data.data.shuffle(seed=20)\n",
    "# data.data = data.data.select(range(2))\n",
    "sampler = CustomBatchSampler(len(data), batch_size=2)\n",
    "dl = DataLoader(data, batch_sampler=sampler, collate_fn=pad_collate_fn)\n",
    "\n",
    "custom_transformer = build_transformer(config)\n",
    "custom_model = custom_transformer.transformer\n",
    "pytorch_model = torch.nn.Transformer(d_model=256, nhead=8, dropout=0.0, batch_first=True)\n",
    "for batch in dl:\n",
    "    src, tgt = batch\n",
    "\n",
    "src = torch.cat((src, torch.zeros(2,2, dtype=torch.long)), dim=1)\n",
    "tgt = torch.cat((tgt, torch.zeros(2,3, dtype=torch.long)), dim=1)\n",
    "src_mask = (src == 0)\n",
    "tgt_mask = (tgt == 0)\n",
    "causal_mask = pytorch_model.generate_square_subsequent_mask(tgt.size(1), dtype=bool)\n",
    "\n",
    "src = custom_transformer.src_word_embedding(src)\n",
    "tgt = custom_transformer.src_word_embedding(tgt)\n",
    "\n",
    "# src = torch.rand(1, 5, 4)\n",
    "# tgt = torch.rand(1, 5, 4)\n",
    "# src_mask = ~torch.tensor([1,1,1,0,0], dtype=bool).unsqueeze(0)\n",
    "# tgt_mask = ~torch.tensor([1,1,1,1,0], dtype=bool).unsqueeze(0)\n",
    "copy_transformer_weights(custom_model, pytorch_model)\n",
    "\n",
    "pytorch_out = pytorch_model(src, tgt, src_key_padding_mask=src_mask, tgt_key_padding_mask=tgt_mask, memory_key_padding_mask=src_mask, tgt_mask=causal_mask, tgt_is_causal=True)\n",
    "custom_out = custom_model(src, tgt, src_mask.unsqueeze(1).unsqueeze(2), tgt_mask.unsqueeze(1).unsqueeze(2) | causal_mask, src_mask.unsqueeze(1).unsqueeze(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e70056b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3518,  0.5350, -0.1408,  ...,  0.8623, -0.5968, -0.7835],\n",
       "         [ 0.2023,  0.5008, -0.2484,  ...,  1.0054, -0.6231, -0.9629],\n",
       "         [ 0.0614,  0.6979,  0.1961,  ...,  1.1849, -0.6526, -1.2559],\n",
       "         ...,\n",
       "         [ 0.1636,  0.8781,  0.0622,  ...,  1.4749, -1.0057, -1.1822],\n",
       "         [ 0.3832,  1.5135,  0.1186,  ...,  0.9393, -0.9194, -1.6305],\n",
       "         [-0.0811,  1.3183,  0.2863,  ...,  1.0324, -1.3263, -1.6134]],\n",
       "\n",
       "        [[ 0.5642,  0.8953, -0.5905,  ...,  0.3664, -0.8350, -1.1837],\n",
       "         [ 0.3323,  0.7225, -0.3582,  ...,  0.4065, -0.7129, -1.5853],\n",
       "         [ 0.4308,  0.7377, -0.5279,  ...,  0.6157, -0.6066, -1.5947],\n",
       "         ...,\n",
       "         [ 0.3337,  1.0413, -0.1129,  ...,  0.7482, -0.8384, -1.6685],\n",
       "         [ 0.3337,  1.0413, -0.1129,  ...,  0.7482, -0.8384, -1.6685],\n",
       "         [ 0.3337,  1.0413, -0.1129,  ...,  0.7482, -0.8384, -1.6685]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ce3e4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3518,  0.5350, -0.1408,  ...,  0.8623, -0.5968, -0.7835],\n",
       "         [ 0.2023,  0.5008, -0.2484,  ...,  1.0054, -0.6231, -0.9629],\n",
       "         [ 0.0614,  0.6979,  0.1961,  ...,  1.1849, -0.6526, -1.2559],\n",
       "         ...,\n",
       "         [ 0.1636,  0.8781,  0.0622,  ...,  1.4749, -1.0057, -1.1822],\n",
       "         [ 0.3832,  1.5135,  0.1186,  ...,  0.9393, -0.9194, -1.6305],\n",
       "         [-0.0811,  1.3183,  0.2863,  ...,  1.0324, -1.3263, -1.6134]],\n",
       "\n",
       "        [[ 0.5642,  0.8953, -0.5905,  ...,  0.3664, -0.8350, -1.1837],\n",
       "         [ 0.3323,  0.7225, -0.3582,  ...,  0.4065, -0.7129, -1.5853],\n",
       "         [ 0.4308,  0.7377, -0.5279,  ...,  0.6157, -0.6066, -1.5947],\n",
       "         ...,\n",
       "         [ 0.3337,  1.0413, -0.1129,  ...,  0.7482, -0.8384, -1.6685],\n",
       "         [ 0.3337,  1.0413, -0.1129,  ...,  0.7482, -0.8384, -1.6685],\n",
       "         [ 0.3337,  1.0413, -0.1129,  ...,  0.7482, -0.8384, -1.6685]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a171a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
