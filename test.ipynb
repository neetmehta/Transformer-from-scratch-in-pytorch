{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138115e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
    "special_token_dict = {\"bos_token\": \"<s>\"}\n",
    "tokenizer.add_special_tokens(special_token_dict)\n",
    "\n",
    "tokenizer.encode(\"hi my name is neet\")\n",
    "import torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class WMTDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path, src_tokenizer, tgt_tokenizer, seq_len):\n",
    "        super().__init__()\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.src_vocab_size = src_tokenizer.vocab_size\n",
    "        self.tgt_vocab_size = tgt_tokenizer.vocab_size\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sos_token = self.src_tokenizer.encode(['<s>'])[0]\n",
    "        eos_token = self.src_tokenizer.encode(['</s>'])[0]\n",
    "        pad_token = self.src_tokenizer.encode(['<pad>'])[0]\n",
    "        src_encoding = self.src_tokenizer.encode(self.data.iloc[index]['en'])[:-1] # remove default eos token\n",
    "        tgt_encoding = self.tgt_tokenizer.encode(self.data.iloc[index]['de'])[:-1] # remove default eos token\n",
    "        print(\"len of src sen: \", len(src_encoding))\n",
    "        print(\"len of tgt sen: \", len(tgt_encoding))\n",
    "        assert len(src_encoding) < self.seq_len + 2, \"sentence too big\"\n",
    "        assert len(tgt_encoding) < self.seq_len + 2, \"sentence too big\"\n",
    "        \n",
    "        src_padding_len = self.seq_len - (len(src_encoding) + 2)  \n",
    "        tgt_padding_len = self.seq_len - (len(tgt_encoding) + 2) \n",
    "        \n",
    "        src_encoding = torch.tensor([sos_token] + src_encoding + [eos_token] + [pad_token]*src_padding_len, dtype=torch.long)\n",
    "        tgt_encoding = torch.tensor([sos_token] + tgt_encoding + [eos_token] + [pad_token]*tgt_padding_len, dtype=torch.long)\n",
    "        \n",
    "        causal_mask = torch.triu(torch.ones(self.seq_len, self.seq_len, dtype=bool), diagonal=1).to(bool)\n",
    "\n",
    "        src_mask = (src_encoding == pad_token).unsqueeze(0)\n",
    "        tgt_mask = (tgt_encoding == pad_token).unsqueeze(0)\n",
    "                \n",
    "        return src_encoding, tgt_encoding, src_mask, tgt_mask\n",
    "    \n",
    "ds = WMTDataset(\"wmt14_translate_de-en_test.csv\", tokenizer, tokenizer, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33cfba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from model import build_transformer, generate_causal_mask\n",
    "from config import TransformerConfig\n",
    "\n",
    "config = TransformerConfig()\n",
    "\n",
    "model = build_transformer(config)\n",
    "\n",
    "dataloader = DataLoader(ds, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a355f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    src, tgt, src_mask, tgt_mask = batch\n",
    "    enc_self_attn_mask = src_mask.unsqueeze(2) | src_mask.unsqueeze(3)\n",
    "    \n",
    "    causal_mask = generate_causal_mask(200)\n",
    "    \n",
    "    dec_self_attn_mask = tgt_mask.unsqueeze(2) | tgt_mask.unsqueeze(3) | causal_mask\n",
    "    \n",
    "    dec_cross_attn_mask = tgt_mask.unsqueeze(3) | src_mask.unsqueeze(2)\n",
    "    \n",
    "    y = model(src, tgt, enc_self_attn_mask, dec_self_attn_mask, dec_cross_attn_mask)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f852df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_cross_attn_mask[0][0,0,34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8091ff64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
