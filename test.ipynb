{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d622909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_attention_weights(custom_attn, torch_attn):\n",
    "    # QKV weights: PyTorch uses one big linear layer for qkv\n",
    "    qkv_weight = torch_attn.self_attn.in_proj_weight\n",
    "    qkv_bias = torch_attn.self_attn.in_proj_bias\n",
    "    d_model = qkv_weight.shape[1]\n",
    "    d_k = d_model // custom_attn.h\n",
    "\n",
    "    # Split into Q, K, V\n",
    "    custom_attn.w_q.weight.data.copy_(qkv_weight[:d_model])\n",
    "    custom_attn.w_k.weight.data.copy_(qkv_weight[d_model:2*d_model])\n",
    "    custom_attn.w_v.weight.data.copy_(qkv_weight[2*d_model:])\n",
    "    \n",
    "    # Output projection\n",
    "    custom_attn.w_o.weight.data.copy_(torch_attn.self_attn.out_proj.weight)\n",
    "\n",
    "def copy_ffn_weights(custom_ffn, torch_ffn):\n",
    "    custom_ffn.linear_1.weight.data.copy_(torch_ffn.linear1.weight)\n",
    "    custom_ffn.linear_1.bias.data.copy_(torch_ffn.linear1.bias)\n",
    "    custom_ffn.linear_2.weight.data.copy_(torch_ffn.linear2.weight)\n",
    "    custom_ffn.linear_2.bias.data.copy_(torch_ffn.linear2.bias)\n",
    "\n",
    "def copy_layer_norm(custom_ln, torch_ln):\n",
    "    custom_ln.weight.data.copy_(torch_ln.weight)\n",
    "    custom_ln.bias.data.copy_(torch_ln.bias)\n",
    "\n",
    "def copy_encoder_weights(custom_encoder, torch_encoder):\n",
    "    for i, (c_layer, t_layer) in enumerate(zip(custom_encoder.encoder_layers, torch_encoder.layers)):\n",
    "        copy_attention_weights(c_layer.multi_headed_self_attention, t_layer)\n",
    "        copy_ffn_weights(c_layer.ffn, t_layer)  # Pass layer1 for dimension reference\n",
    "        copy_layer_norm(c_layer.norm1, t_layer.norm1)\n",
    "        copy_layer_norm(c_layer.norm2, t_layer.norm2)\n",
    "\n",
    "def copy_decoder_weights(custom_decoder, torch_decoder):\n",
    "    for i, (c_layer, t_layer) in enumerate(zip(custom_decoder.decoder_layers, torch_decoder.layers)):\n",
    "        # Masked self-attention\n",
    "        qkv_weight = t_layer.self_attn.in_proj_weight\n",
    "        d_model = qkv_weight.shape[1]\n",
    "        c_layer.masked_multi_headed_self_attention.w_q.weight.data.copy_(qkv_weight[:d_model])\n",
    "        c_layer.masked_multi_headed_self_attention.w_k.weight.data.copy_(qkv_weight[d_model:2*d_model])\n",
    "        c_layer.masked_multi_headed_self_attention.w_v.weight.data.copy_(qkv_weight[2*d_model:])\n",
    "        c_layer.masked_multi_headed_self_attention.w_o.weight.data.copy_(t_layer.self_attn.out_proj.weight)\n",
    "\n",
    "        # Cross-attention\n",
    "        cross_qkv_weight = t_layer.multihead_attn.in_proj_weight\n",
    "        c_layer.multi_headed_cross_attention.w_q.weight.data.copy_(cross_qkv_weight[:d_model])\n",
    "        c_layer.multi_headed_cross_attention.w_k.weight.data.copy_(cross_qkv_weight[d_model:2*d_model])\n",
    "        c_layer.multi_headed_cross_attention.w_v.weight.data.copy_(cross_qkv_weight[2*d_model:])\n",
    "        c_layer.multi_headed_cross_attention.w_o.weight.data.copy_(t_layer.multihead_attn.out_proj.weight)\n",
    "\n",
    "        # FFN and LayerNorm\n",
    "        copy_ffn_weights(c_layer.ffn, t_layer.linear1)\n",
    "        copy_layer_norm(c_layer.norm1, t_layer.norm1)\n",
    "        copy_layer_norm(c_layer.norm2, t_layer.norm2)\n",
    "        copy_layer_norm(c_layer.norm3, t_layer.norm3)\n",
    "\n",
    "def copy_transformer_weights(custom_transformer, torch_transformer):\n",
    "    copy_encoder_weights(custom_transformer.encoder, torch_transformer.encoder)\n",
    "    copy_decoder_weights(custom_transformer.decoder, torch_transformer.decoder)\n",
    "def copy_attention_weights(custom_attn, torch_attn):\n",
    "    # QKV weights: PyTorch uses one big linear layer for qkv\n",
    "    qkv_weight = torch_attn.self_attn.in_proj_weight\n",
    "    qkv_bias = torch_attn.self_attn.in_proj_bias\n",
    "    d_model = qkv_weight.shape[1]\n",
    "    d_k = d_model // custom_attn.h\n",
    "\n",
    "    # Split into Q, K, V\n",
    "    custom_attn.w_q.weight.data.copy_(qkv_weight[:d_model])\n",
    "    custom_attn.w_k.weight.data.copy_(qkv_weight[d_model:2*d_model])\n",
    "    custom_attn.w_v.weight.data.copy_(qkv_weight[2*d_model:])\n",
    "    \n",
    "    # Output projection\n",
    "    custom_attn.w_o.weight.data.copy_(torch_attn.self_attn.out_proj.weight)\n",
    "\n",
    "def copy_ffn_weights(custom_ffn, torch_ffn):\n",
    "    custom_ffn.linear_1.weight.data.copy_(torch_ffn.linear1.weight)\n",
    "    custom_ffn.linear_1.bias.data.copy_(torch_ffn.linear1.bias)\n",
    "    custom_ffn.linear_2.weight.data.copy_(torch_ffn.linear2.weight)\n",
    "    custom_ffn.linear_2.bias.data.copy_(torch_ffn.linear2.bias)\n",
    "\n",
    "def copy_layer_norm(custom_ln, torch_ln):\n",
    "    custom_ln.weight.data.copy_(torch_ln.weight)\n",
    "    custom_ln.bias.data.copy_(torch_ln.bias)\n",
    "\n",
    "def copy_encoder_weights(custom_encoder, torch_encoder):\n",
    "    for i, (c_layer, t_layer) in enumerate(zip(custom_encoder.encoder_layers, torch_encoder.layers)):\n",
    "        copy_attention_weights(c_layer.multi_headed_self_attention, t_layer)\n",
    "        copy_ffn_weights(c_layer.ffn, t_layer)  # Pass layer1 for dimension reference\n",
    "        copy_layer_norm(c_layer.norm1, t_layer.norm1)\n",
    "        copy_layer_norm(c_layer.norm2, t_layer.norm2)\n",
    "\n",
    "def copy_decoder_weights(custom_decoder, torch_decoder):\n",
    "    for i, (c_layer, t_layer) in enumerate(zip(custom_decoder.decoder_layers, torch_decoder.layers)):\n",
    "        # Masked self-attention\n",
    "        qkv_weight = t_layer.self_attn.in_proj_weight\n",
    "        d_model = qkv_weight.shape[1]\n",
    "        c_layer.masked_multi_headed_self_attention.w_q.weight.data.copy_(qkv_weight[:d_model])\n",
    "        c_layer.masked_multi_headed_self_attention.w_k.weight.data.copy_(qkv_weight[d_model:2*d_model])\n",
    "        c_layer.masked_multi_headed_self_attention.w_v.weight.data.copy_(qkv_weight[2*d_model:])\n",
    "        c_layer.masked_multi_headed_self_attention.w_o.weight.data.copy_(t_layer.self_attn.out_proj.weight)\n",
    "\n",
    "        # Cross-attention\n",
    "        cross_qkv_weight = t_layer.multihead_attn.in_proj_weight\n",
    "        c_layer.multi_headed_cross_attention.w_q.weight.data.copy_(cross_qkv_weight[:d_model])\n",
    "        c_layer.multi_headed_cross_attention.w_k.weight.data.copy_(cross_qkv_weight[d_model:2*d_model])\n",
    "        c_layer.multi_headed_cross_attention.w_v.weight.data.copy_(cross_qkv_weight[2*d_model:])\n",
    "        c_layer.multi_headed_cross_attention.w_o.weight.data.copy_(t_layer.multihead_attn.out_proj.weight)\n",
    "\n",
    "        # FFN and LayerNorm\n",
    "        copy_ffn_weights(c_layer.ffn, t_layer)\n",
    "        copy_layer_norm(c_layer.norm1, t_layer.norm1)\n",
    "        copy_layer_norm(c_layer.norm2, t_layer.norm2)\n",
    "        copy_layer_norm(c_layer.norm3, t_layer.norm3)\n",
    "\n",
    "def copy_transformer_weights(custom_transformer, torch_transformer):\n",
    "    copy_encoder_weights(custom_transformer.encoder, torch_transformer.encoder)\n",
    "    copy_decoder_weights(custom_transformer.decoder, torch_transformer.decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "920cffe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wmt14 (C:\\Users\\neetm\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\3d7d25048da28a2f2a8dda5ca306bdc4affcf02bbcb3d277cfe2d5d7b1d71ebc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4aa8261d0c042d492481236a55bcab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from configs.transformer_tiny import Config\n",
    "from tokenizer import TokenizerWrapper\n",
    "from data import WMTENDE, CustomBatchSampler, pad_collate_fn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from model import build_transformer\n",
    "\n",
    "def copy_attention_weights(custom_attn, torch_attn):\n",
    "    # QKV weights: PyTorch uses one big linear layer for qkv\n",
    "    qkv_weight = torch_attn.self_attn.in_proj_weight\n",
    "    qkv_bias = torch_attn.self_attn.in_proj_bias\n",
    "    d_model = qkv_weight.shape[1]\n",
    "    d_k = d_model // custom_attn.h\n",
    "\n",
    "    # Split into Q, K, V\n",
    "    custom_attn.w_q.weight.data.copy_(qkv_weight[:d_model])\n",
    "    custom_attn.w_k.weight.data.copy_(qkv_weight[d_model:2*d_model])\n",
    "    custom_attn.w_v.weight.data.copy_(qkv_weight[2*d_model:])\n",
    "\n",
    "    # Output projection\n",
    "    custom_attn.w_o.weight.data.copy_(torch_attn.self_attn.out_proj.weight)\n",
    "\n",
    "def copy_ffn_weights(custom_ffn, torch_ffn):\n",
    "    custom_ffn.linear_1.weight.data.copy_(torch_ffn.linear1.weight)\n",
    "    custom_ffn.linear_1.bias.data.copy_(torch_ffn.linear1.bias)\n",
    "    custom_ffn.linear_2.weight.data.copy_(torch_ffn.linear2.weight)\n",
    "    custom_ffn.linear_2.bias.data.copy_(torch_ffn.linear2.bias)\n",
    "\n",
    "def copy_layer_norm(custom_ln, torch_ln):\n",
    "    custom_ln.weight.data.copy_(torch_ln.weight)\n",
    "    custom_ln.bias.data.copy_(torch_ln.bias)\n",
    "\n",
    "def copy_encoder_weights(custom_encoder, torch_encoder):\n",
    "    for i, (c_layer, t_layer) in enumerate(zip(custom_encoder.encoder_layers, torch_encoder.layers)):\n",
    "        copy_attention_weights(c_layer.multi_headed_self_attention, t_layer)\n",
    "        copy_ffn_weights(c_layer.ffn, t_layer)  # Pass layer1 for dimension reference\n",
    "        copy_layer_norm(c_layer.norm1, t_layer.norm1)\n",
    "        copy_layer_norm(c_layer.norm2, t_layer.norm2)\n",
    "\n",
    "def copy_decoder_weights(custom_decoder, torch_decoder):\n",
    "    for i, (c_layer, t_layer) in enumerate(zip(custom_decoder.decoder_layers, torch_decoder.layers)):\n",
    "        # Masked self-attention\n",
    "        qkv_weight = t_layer.self_attn.in_proj_weight\n",
    "        d_model = qkv_weight.shape[1]\n",
    "        c_layer.masked_multi_headed_self_attention.w_q.weight.data.copy_(qkv_weight[:d_model])\n",
    "        c_layer.masked_multi_headed_self_attention.w_k.weight.data.copy_(qkv_weight[d_model:2*d_model])\n",
    "        c_layer.masked_multi_headed_self_attention.w_v.weight.data.copy_(qkv_weight[2*d_model:])\n",
    "        c_layer.masked_multi_headed_self_attention.w_o.weight.data.copy_(t_layer.self_attn.out_proj.weight)\n",
    "\n",
    "        # Cross-attention\n",
    "        cross_qkv_weight = t_layer.multihead_attn.in_proj_weight\n",
    "        c_layer.multi_headed_cross_attention.w_q.weight.data.copy_(cross_qkv_weight[:d_model])\n",
    "        c_layer.multi_headed_cross_attention.w_k.weight.data.copy_(cross_qkv_weight[d_model:2*d_model])\n",
    "        c_layer.multi_headed_cross_attention.w_v.weight.data.copy_(cross_qkv_weight[2*d_model:])\n",
    "        c_layer.multi_headed_cross_attention.w_o.weight.data.copy_(t_layer.multihead_attn.out_proj.weight)\n",
    "\n",
    "        # FFN and LayerNorm\n",
    "        copy_ffn_weights(c_layer.ffn, t_layer.linear1)\n",
    "        copy_layer_norm(c_layer.norm1, t_layer.norm1)\n",
    "        copy_layer_norm(c_layer.norm2, t_layer.norm2)\n",
    "        copy_layer_norm(c_layer.norm3, t_layer.norm3)\n",
    "\n",
    "def copy_transformer_weights(custom_transformer, torch_transformer):\n",
    "    copy_encoder_weights(custom_transformer.encoder, torch_transformer.encoder)\n",
    "    copy_decoder_weights(custom_transformer.decoder, torch_transformer.decoder)\n",
    "def copy_attention_weights(custom_attn, torch_attn):\n",
    "    # QKV weights: PyTorch uses one big linear layer for qkv\n",
    "    qkv_weight = torch_attn.self_attn.in_proj_weight\n",
    "    qkv_bias = torch_attn.self_attn.in_proj_bias\n",
    "    d_model = qkv_weight.shape[1]\n",
    "    d_k = d_model // custom_attn.h\n",
    "\n",
    "    # Split into Q, K, V\n",
    "    custom_attn.w_q.weight.data.copy_(qkv_weight[:d_model])\n",
    "    custom_attn.w_k.weight.data.copy_(qkv_weight[d_model:2*d_model])\n",
    "    custom_attn.w_v.weight.data.copy_(qkv_weight[2*d_model:])\n",
    "\n",
    "    # Output projection\n",
    "    custom_attn.w_o.weight.data.copy_(torch_attn.self_attn.out_proj.weight)\n",
    "\n",
    "def copy_ffn_weights(custom_ffn, torch_ffn):\n",
    "    custom_ffn.linear_1.weight.data.copy_(torch_ffn.linear1.weight)\n",
    "    custom_ffn.linear_1.bias.data.copy_(torch_ffn.linear1.bias)\n",
    "    custom_ffn.linear_2.weight.data.copy_(torch_ffn.linear2.weight)\n",
    "    custom_ffn.linear_2.bias.data.copy_(torch_ffn.linear2.bias)\n",
    "\n",
    "def copy_layer_norm(custom_ln, torch_ln):\n",
    "    custom_ln.weight.data.copy_(torch_ln.weight)\n",
    "    custom_ln.bias.data.copy_(torch_ln.bias)\n",
    "\n",
    "def copy_encoder_weights(custom_encoder, torch_encoder):\n",
    "    for i, (c_layer, t_layer) in enumerate(zip(custom_encoder.encoder_layers, torch_encoder.layers)):\n",
    "        copy_attention_weights(c_layer.multi_headed_self_attention, t_layer)\n",
    "        copy_ffn_weights(c_layer.ffn, t_layer)  # Pass layer1 for dimension reference\n",
    "        copy_layer_norm(c_layer.norm1, t_layer.norm1)\n",
    "        copy_layer_norm(c_layer.norm2, t_layer.norm2)\n",
    "\n",
    "def copy_decoder_weights(custom_decoder, torch_decoder):\n",
    "    for i, (c_layer, t_layer) in enumerate(zip(custom_decoder.decoder_layers, torch_decoder.layers)):\n",
    "        # Masked self-attention\n",
    "        qkv_weight = t_layer.self_attn.in_proj_weight\n",
    "        d_model = qkv_weight.shape[1]\n",
    "        c_layer.masked_multi_headed_self_attention.w_q.weight.data.copy_(qkv_weight[:d_model])\n",
    "        c_layer.masked_multi_headed_self_attention.w_k.weight.data.copy_(qkv_weight[d_model:2*d_model])\n",
    "        c_layer.masked_multi_headed_self_attention.w_v.weight.data.copy_(qkv_weight[2*d_model:])\n",
    "        c_layer.masked_multi_headed_self_attention.w_o.weight.data.copy_(t_layer.self_attn.out_proj.weight)\n",
    "\n",
    "        # Cross-attention\n",
    "        cross_qkv_weight = t_layer.multihead_attn.in_proj_weight\n",
    "        c_layer.multi_headed_cross_attention.w_q.weight.data.copy_(cross_qkv_weight[:d_model])\n",
    "        c_layer.multi_headed_cross_attention.w_k.weight.data.copy_(cross_qkv_weight[d_model:2*d_model])\n",
    "        c_layer.multi_headed_cross_attention.w_v.weight.data.copy_(cross_qkv_weight[2*d_model:])\n",
    "        c_layer.multi_headed_cross_attention.w_o.weight.data.copy_(t_layer.multihead_attn.out_proj.weight)\n",
    "\n",
    "        # FFN and LayerNorm\n",
    "        copy_ffn_weights(c_layer.ffn, t_layer)\n",
    "        copy_layer_norm(c_layer.norm1, t_layer.norm1)\n",
    "        copy_layer_norm(c_layer.norm2, t_layer.norm2)\n",
    "        copy_layer_norm(c_layer.norm3, t_layer.norm3)\n",
    "\n",
    "def copy_transformer_weights(custom_transformer, torch_transformer):\n",
    "    copy_encoder_weights(custom_transformer.encoder, torch_transformer.encoder)\n",
    "    copy_decoder_weights(custom_transformer.decoder, torch_transformer.decoder)\n",
    "\n",
    "config = Config()\n",
    "tokenizer = TokenizerWrapper(config)\n",
    "data = WMTENDE(config, tokenizer, 'test')\n",
    "data.data.shuffle(seed=20)\n",
    "# data.data = data.data.select(range(2))\n",
    "sampler = CustomBatchSampler(len(data), batch_size=2)\n",
    "dl = DataLoader(data, batch_sampler=sampler, collate_fn=pad_collate_fn)\n",
    "\n",
    "custom_transformer = build_transformer(config)\n",
    "custom_model = custom_transformer.transformer\n",
    "pytorch_model = torch.nn.Transformer(d_model=256, nhead=8, dropout=0.0, batch_first=True)\n",
    "for batch in dl:\n",
    "    src, tgt = batch\n",
    "\n",
    "src = torch.cat((src, torch.zeros(2,2, dtype=torch.long)), dim=1)\n",
    "tgt = torch.cat((tgt, torch.zeros(2,3, dtype=torch.long)), dim=1)\n",
    "src_mask = (src == 0)\n",
    "tgt_mask = (tgt == 0)\n",
    "causal_mask = pytorch_model.generate_square_subsequent_mask(tgt.size(1), dtype=bool)\n",
    "\n",
    "\n",
    "model_src = src\n",
    "model_tgt= tgt\n",
    "\n",
    "src = custom_transformer.src_word_embedding(src) * math.sqrt(custom_transformer.src_word_embedding.d_model)\n",
    "src = custom_transformer.positional_encoding(src)\n",
    "\n",
    "tgt = custom_transformer.tgt_word_embedding(tgt) * math.sqrt(custom_transformer.tgt_word_embedding.d_model)\n",
    "tgt = custom_transformer.positional_encoding(tgt)\n",
    "\n",
    "# src = torch.rand(1, 5, 4)\n",
    "# tgt = torch.rand(1, 5, 4)\n",
    "# src_mask = ~torch.tensor([1,1,1,0,0], dtype=bool).unsqueeze(0)\n",
    "# tgt_mask = ~torch.tensor([1,1,1,1,0], dtype=bool).unsqueeze(0)\n",
    "copy_transformer_weights(custom_model, pytorch_model)\n",
    "\n",
    "pytorch_out = pytorch_model(src, tgt, src_key_padding_mask=src_mask, tgt_key_padding_mask=tgt_mask, memory_key_padding_mask=src_mask, tgt_mask=causal_mask, tgt_is_causal=True)\n",
    "custom_out = custom_model(src, tgt, src_mask.unsqueeze(1).unsqueeze(2), tgt_mask.unsqueeze(1).unsqueeze(2) | causal_mask, src_mask.unsqueeze(1).unsqueeze(2))\n",
    "model_out = custom_transformer(model_src, model_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1dc8a4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wmt14 (C:\\Users\\neetm\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\3d7d25048da28a2f2a8dda5ca306bdc4affcf02bbcb3d277cfe2d5d7b1d71ebc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226b4473e4544174bc7994503f726446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "dataset = load_dataset(config.dataset, config.language)\n",
    "combined_dataset = concatenate_datasets([\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1ee98e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translation'],\n",
       "    num_rows: 4514788\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f0362d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
