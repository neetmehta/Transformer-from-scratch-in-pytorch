{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138115e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
    "special_token_dict = {\"bos_token\": \"<s>\"}\n",
    "tokenizer.add_special_tokens(special_token_dict)\n",
    "\n",
    "tokenizer.encode(\"hi my name is neet\")\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class WMTDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path, src_tokenizer, tgt_tokenizer, seq_len):\n",
    "        super().__init__()\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.src_vocab_size = src_tokenizer.vocab_size\n",
    "        self.tgt_vocab_size = tgt_tokenizer.vocab_size\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sos_token = self.src_tokenizer.encode(['<s>'])[0]\n",
    "        eos_token = self.src_tokenizer.encode(['</s>'])[0]\n",
    "        pad_token = self.src_tokenizer.encode(['<pad>'])[0]\n",
    "        src_encoding = self.src_tokenizer.encode(self.data.iloc[index]['en'])[:-1] # remove default eos token\n",
    "        tgt_encoding = self.tgt_tokenizer.encode(self.data.iloc[index]['de'])[:-1] # remove default eos token\n",
    "        print(\"len of src sen: \", len(src_encoding))\n",
    "        print(\"len of tgt sen: \", len(tgt_encoding))\n",
    "        assert len(src_encoding) < self.seq_len + 2, \"sentence too big\"\n",
    "        assert len(tgt_encoding) < self.seq_len + 2, \"sentence too big\"\n",
    "        \n",
    "        src_padding_len = self.seq_len - (len(src_encoding) + 2)  \n",
    "        tgt_padding_len = self.seq_len - (len(tgt_encoding) + 2) \n",
    "        \n",
    "        src_encoding = torch.tensor([sos_token] + src_encoding + [eos_token] + [pad_token]*src_padding_len, dtype=torch.uint64)\n",
    "        tgt_encoding = torch.tensor([sos_token] + tgt_encoding + [eos_token] + [pad_token]*tgt_padding_len, dtype=torch.uint64)\n",
    "        \n",
    "        causal_mask = torch.triu(torch.ones(self.seq_len, self.seq_len, dtype=bool), diagonal=1).to(bool)\n",
    "\n",
    "        encoder_self_attention_mask = (src_encoding == pad_token).unsqueeze(0) | (src_encoding == pad_token).unsqueeze(1)\n",
    "        decoder_self_attention_mask = (tgt_encoding == pad_token).unsqueeze(0) | (tgt_encoding == pad_token).unsqueeze(1) | causal_mask\n",
    "        decoder_cross_attention_mask = (tgt_encoding == pad_token).unsqueeze(0) | (src_encoding == pad_token).unsqueeze(1)\n",
    "        \n",
    "        return src_encoding, tgt_encoding, encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n",
    "    \n",
    "ds = WMTDataset(\"wmt14_translate_de-en_test.csv\", tokenizer, tokenizer, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4164a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de28c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(4,4)\n",
    "y = torch.tensor([[1,0,0,1]], dtype=bool)\n",
    "x.masked_fill_(y, -1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8fd6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2,5,2)\n",
    "y = torch.randint(0,2,(1,2)).to(bool)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d2c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.masked_fill_(y, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f8c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "class WordEmbeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.word_embd = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        \n",
    "        x = self.word_embd(tokens)\n",
    "        return x\n",
    "    \n",
    "class PositionalEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, seq_len, d_model):\n",
    "        super().__init__()\n",
    " \n",
    "        pe = torch.zeros(seq_len, d_model, requires_grad=False)\n",
    "        div_term = torch.pow(10000, torch.arange(0, d_model, 2)/d_model)\n",
    "        pos = torch.arange(seq_len).unsqueeze(1)\n",
    "        pe[:, 0::2] = torch.sin(pos/div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos/div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return x + self.pe[:, :x.shape[1], :]\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.h = num_heads\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model not divisible by num_heads\"\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        B, seq_len, embd_dim = q.shape\n",
    "        # q -> (B, seq_len, embd_dim)\n",
    "        # k -> (B, seq_len, embd_dim)\n",
    "        # v -> (B, seq_len, embd_dim)\n",
    "        queries = self.w_q(q)\n",
    "        keys = self.w_k(k)\n",
    "        values = self.w_v(v)\n",
    "        # q -> (B, seq_len, embd_dim)\n",
    "        # k -> (B, seq_len, embd_dim)\n",
    "        # v -> (B, seq_len, embd_dim)\n",
    "        \n",
    "        queries = queries.view(B, seq_len, self.h, self.d_k).transpose(1,2)\n",
    "        keys = keys.view(B, seq_len, self.h, self.d_k).transpose(1,2)\n",
    "        values = values.view(B, seq_len, self.h, self.d_k).transpose(1,2)\n",
    "        \n",
    "        # q -> (B, h, seq_len, dk)\n",
    "        # k -> (B, h, seq_len, dk)\n",
    "        # v -> (B, h, seq_len, dk)\n",
    "        \n",
    "        attention_weights = (queries @ keys.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            # boolean mask\n",
    "            attention_weights = attention_weights.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        attention_weights = attention_weights.softmax(dim=-1)\n",
    "        \n",
    "        # attention_weights -> (B, h, seq_len, seq_len)\n",
    "        \n",
    "        attention_output = attention_weights @ values\n",
    "        \n",
    "        # attention_weights -> (B, h, seq_len, dk)\n",
    "        \n",
    "        attention_output = attention_output.transpose(1, 2).reshape(B, seq_len, -1)\n",
    "        \n",
    "        attention_output = self.w_o(attention_output)\n",
    "        return attention_output, attention_weights\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, p_d=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # p_d = 0.1 from original paper\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(p_d)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.relu(self.dropout(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "    \n",
    "class EncoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, p_d=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p_d)\n",
    "        self.multi_headed_self_attention = MultiheadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.ffn = FeedForwardNetwork(d_model=d_model, d_ff=d_ff, p_d=p_d)\n",
    "        \n",
    "    def forward(self, x, self_attn_mask):\n",
    "        \n",
    "        x_residual_1 = x\n",
    "        \n",
    "        attention_out = self.dropout(self.multi_headed_self_attention(x, x, x, self_attn_mask)[0])\n",
    "        \n",
    "        sublayer_1_out = self.layer_norm(x_residual_1 + attention_out)\n",
    "        \n",
    "        x_residual_2 = sublayer_1_out\n",
    "        \n",
    "        ffn_out = self.dropout(self.ffn(sublayer_1_out))\n",
    "        \n",
    "        sublayer_2_out = self.layer_norm(x_residual_2 + ffn_out)\n",
    "        \n",
    "        return sublayer_2_out\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, p_d=0.0, layer_norm_eps=1e-5, bias=True):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps, bias=bias)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps, bias=bias)\n",
    "        self.norm3 = nn.LayerNorm(d_model, eps=layer_norm_eps, bias=bias)\n",
    "        self.dropout1 = nn.Dropout(p_d)\n",
    "        self.dropout2 = nn.Dropout(p_d)\n",
    "        self.dropout3 = nn.Dropout(p_d)\n",
    "        self.masked_multi_headed_self_attention = MultiheadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.multi_headed_cross_attention = MultiheadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.ffn = FeedForwardNetwork(d_model=d_model, d_ff=d_ff, p_d=p_d)\n",
    "        \n",
    "    def forward(self, x, memory, masked_self_attn_mask, cross_attn_mask):\n",
    "        print(self.masked_multi_headed_self_attention(x, x, x, masked_self_attn_mask)[0])\n",
    "        x = self.norm1(x + self.dropout1(self.masked_multi_headed_self_attention(x, x, x, masked_self_attn_mask)[0]))\n",
    "        \n",
    "        x = self.norm2(x + self.dropout2(self.multi_headed_cross_attention(x, memory, memory, cross_attn_mask)[0]))\n",
    "        \n",
    "        x = self.norm3(x + self.dropout3(self.ffn(x)))\n",
    "        \n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a377e06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3731,  0.5099,  0.4786,  0.0406,  0.1351, -0.0435, -0.1737,\n",
      "          -0.7356],\n",
      "         [-0.3703,  0.5110,  0.4788,  0.0455,  0.1395, -0.0410, -0.1781,\n",
      "          -0.7381],\n",
      "         [-0.3719,  0.5099,  0.4792,  0.0426,  0.1367, -0.0425, -0.1757,\n",
      "          -0.7360],\n",
      "         [-0.3704,  0.5129,  0.4783,  0.0455,  0.1409, -0.0408, -0.1780,\n",
      "          -0.7404],\n",
      "         [-0.3721,  0.5115,  0.4782,  0.0419,  0.1371, -0.0427, -0.1747,\n",
      "          -0.7377]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.3731,  0.5099,  0.4786,  0.0406,  0.1351, -0.0435, -0.1737,\n",
      "          -0.7356],\n",
      "         [-0.3703,  0.5110,  0.4788,  0.0455,  0.1395, -0.0410, -0.1781,\n",
      "          -0.7381],\n",
      "         [-0.3719,  0.5099,  0.4792,  0.0426,  0.1367, -0.0425, -0.1757,\n",
      "          -0.7360],\n",
      "         [-0.3704,  0.5129,  0.4783,  0.0455,  0.1409, -0.0408, -0.1780,\n",
      "          -0.7404],\n",
      "         [-0.3721,  0.5115,  0.4782,  0.0419,  0.1371, -0.0427, -0.1747,\n",
      "          -0.7377]]], grad_fn=<TransposeBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "x = torch.rand(1, 5, 8)  # (batch, seq_len, d_model)\n",
    "\n",
    "m1 = DecoderBlock(8, 2, 16, p_d=0.0)\n",
    "m2 = nn.TransformerDecoderLayer(8, 2, 16, dropout=0.0, activation='relu', batch_first=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Self-attention weights\n",
    "    m2.self_attn.in_proj_weight.copy_(\n",
    "        torch.cat([\n",
    "            m1.masked_multi_headed_self_attention.w_q.weight,\n",
    "            m1.masked_multi_headed_self_attention.w_k.weight,\n",
    "            m1.masked_multi_headed_self_attention.w_v.weight\n",
    "        ], dim=0)\n",
    "    )\n",
    "    m2.self_attn.in_proj_bias.copy_(\n",
    "        torch.cat([\n",
    "            m1.masked_multi_headed_self_attention.w_q.bias,\n",
    "            m1.masked_multi_headed_self_attention.w_k.bias,\n",
    "            m1.masked_multi_headed_self_attention.w_v.bias\n",
    "        ], dim=0)\n",
    "    )\n",
    "    m2.self_attn.out_proj.weight.copy_(m1.masked_multi_headed_self_attention.w_o.weight)\n",
    "    m2.self_attn.out_proj.bias.copy_(m1.masked_multi_headed_self_attention.w_o.bias)\n",
    "\n",
    "    # Cross attention weights\n",
    "    m2.multihead_attn.in_proj_weight.copy_(\n",
    "        torch.cat([\n",
    "            m1.multi_headed_cross_attention.w_q.weight,\n",
    "            m1.multi_headed_cross_attention.w_k.weight,\n",
    "            m1.multi_headed_cross_attention.w_v.weight\n",
    "        ], dim=0)\n",
    "    )\n",
    "    m2.multihead_attn.in_proj_bias.copy_(\n",
    "        torch.cat([\n",
    "            m1.multi_headed_cross_attention.w_q.bias,\n",
    "            m1.multi_headed_cross_attention.w_k.bias,\n",
    "            m1.multi_headed_cross_attention.w_v.bias\n",
    "        ], dim=0)\n",
    "    )\n",
    "    m2.multihead_attn.out_proj.weight.copy_(m1.multi_headed_cross_attention.w_o.weight)\n",
    "    m2.multihead_attn.out_proj.bias.copy_(m1.multi_headed_cross_attention.w_o.bias)\n",
    "\n",
    "    # Feed-forward layers\n",
    "    m2.linear1.weight.copy_(m1.ffn.linear_1.weight)\n",
    "    m2.linear1.bias.copy_(m1.ffn.linear_1.bias)\n",
    "    m2.linear2.weight.copy_(m1.ffn.linear_2.weight)\n",
    "    m2.linear2.bias.copy_(m1.ffn.linear_2.bias)\n",
    "\n",
    "    # LayerNorms\n",
    "    m2.norm1.weight.copy_(m1.norm1.weight)\n",
    "    m2.norm1.bias.copy_(m1.norm1.bias)\n",
    "    m2.norm2.weight.copy_(m1.norm2.weight)\n",
    "    m2.norm2.bias.copy_(m1.norm2.bias)\n",
    "    m2.norm3.weight.copy_(m1.norm3.weight)\n",
    "    m2.norm3.bias.copy_(m1.norm3.bias)\n",
    "\n",
    "\n",
    "# Disable dropout randomness\n",
    "m1.eval()\n",
    "m2.eval()\n",
    "\n",
    "out1 = m1(x, x, None, None)\n",
    "\n",
    "out2 = m2(x, x, tgt_mask=None, memory_mask=None)\n",
    "\n",
    "print(torch.allclose(out1, out2, atol=1e-5))  # Should be True!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b8ab38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = MultiheadAttention(8, 2)\n",
    "\n",
    "m2 = nn.MultiheadAttention(8, 2, 0.0, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8eb33f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['in_proj_weight', 'in_proj_bias', 'out_proj.weight', 'out_proj.bias'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f59d2c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0909,  0.1994,  0.1285,  0.2801, -0.1324,  0.1190,  0.1258, -0.2944])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "m2.in_proj_weight.data.copy_(torch.cat([m1.w_q.weight.data, m1.w_k.weight.data, m1.w_v.weight.data]))\n",
    "m2.in_proj_bias.data.copy_(torch.cat([m1.w_q.bias.data, m1.w_k.bias.data, m1.w_v.bias.data]))\n",
    "m2.out_proj.weight.data.copy_(m1.w_o.weight.data)\n",
    "m2.out_proj.bias.data.copy_(m1.w_o.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "91cb7cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(42)\n",
    "with torch.no_grad():\n",
    "    m1.eval()\n",
    "    m2.eval()\n",
    "\n",
    "    x = torch.rand(1, 5, 8)\n",
    "\n",
    "    y = m1(x,x,x,None)[0]\n",
    "    z = m2(x,x,x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9970a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.9802e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          -2.9802e-08,  2.9802e-08,  2.9802e-08],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 2.9802e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.9802e-08,\n",
       "          -2.9802e-08, -2.9802e-08,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y-z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea199552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
