{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d622909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_attention_weights(custom_attn, torch_attn):\n",
    "    # QKV weights: PyTorch uses one big linear layer for qkv\n",
    "    qkv_weight = torch_attn.self_attn.in_proj_weight\n",
    "    qkv_bias = torch_attn.self_attn.in_proj_bias\n",
    "    d_model = qkv_weight.shape[1]\n",
    "    d_k = d_model // custom_attn.h\n",
    "\n",
    "    # Split into Q, K, V\n",
    "    custom_attn.w_q.weight.data.copy_(qkv_weight[:d_model])\n",
    "    custom_attn.w_k.weight.data.copy_(qkv_weight[d_model:2*d_model])\n",
    "    custom_attn.w_v.weight.data.copy_(qkv_weight[2*d_model:])\n",
    "    \n",
    "    # Output projection\n",
    "    custom_attn.w_o.weight.data.copy_(torch_attn.self_attn.out_proj.weight)\n",
    "\n",
    "def copy_ffn_weights(custom_ffn, torch_ffn):\n",
    "    custom_ffn.linear_1.weight.data.copy_(torch_ffn.linear1.weight)\n",
    "    custom_ffn.linear_1.bias.data.copy_(torch_ffn.linear1.bias)\n",
    "    custom_ffn.linear_2.weight.data.copy_(torch_ffn.linear2.weight)\n",
    "    custom_ffn.linear_2.bias.data.copy_(torch_ffn.linear2.bias)\n",
    "\n",
    "def copy_layer_norm(custom_ln, torch_ln):\n",
    "    custom_ln.weight.data.copy_(torch_ln.weight)\n",
    "    custom_ln.bias.data.copy_(torch_ln.bias)\n",
    "\n",
    "def copy_encoder_weights(custom_encoder, torch_encoder):\n",
    "    for i, (c_layer, t_layer) in enumerate(zip(custom_encoder.encoder_layers, torch_encoder.layers)):\n",
    "        copy_attention_weights(c_layer.multi_headed_self_attention, t_layer)\n",
    "        copy_ffn_weights(c_layer.ffn, t_layer)  # Pass layer1 for dimension reference\n",
    "        copy_layer_norm(c_layer.norm1, t_layer.norm1)\n",
    "        copy_layer_norm(c_layer.norm2, t_layer.norm2)\n",
    "\n",
    "def copy_decoder_weights(custom_decoder, torch_decoder):\n",
    "    for i, (c_layer, t_layer) in enumerate(zip(custom_decoder.decoder_layers, torch_decoder.layers)):\n",
    "        # Masked self-attention\n",
    "        qkv_weight = t_layer.self_attn.in_proj_weight\n",
    "        d_model = qkv_weight.shape[1]\n",
    "        c_layer.masked_multi_headed_self_attention.w_q.weight.data.copy_(qkv_weight[:d_model])\n",
    "        c_layer.masked_multi_headed_self_attention.w_k.weight.data.copy_(qkv_weight[d_model:2*d_model])\n",
    "        c_layer.masked_multi_headed_self_attention.w_v.weight.data.copy_(qkv_weight[2*d_model:])\n",
    "        c_layer.masked_multi_headed_self_attention.w_o.weight.data.copy_(t_layer.self_attn.out_proj.weight)\n",
    "\n",
    "        # Cross-attention\n",
    "        cross_qkv_weight = t_layer.multihead_attn.in_proj_weight\n",
    "        c_layer.multi_headed_cross_attention.w_q.weight.data.copy_(cross_qkv_weight[:d_model])\n",
    "        c_layer.multi_headed_cross_attention.w_k.weight.data.copy_(cross_qkv_weight[d_model:2*d_model])\n",
    "        c_layer.multi_headed_cross_attention.w_v.weight.data.copy_(cross_qkv_weight[2*d_model:])\n",
    "        c_layer.multi_headed_cross_attention.w_o.weight.data.copy_(t_layer.multihead_attn.out_proj.weight)\n",
    "\n",
    "        # FFN and LayerNorm\n",
    "        copy_ffn_weights(c_layer.ffn, t_layer.linear1)\n",
    "        copy_layer_norm(c_layer.norm1, t_layer.norm1)\n",
    "        copy_layer_norm(c_layer.norm2, t_layer.norm2)\n",
    "        copy_layer_norm(c_layer.norm3, t_layer.norm3)\n",
    "\n",
    "def copy_transformer_weights(custom_transformer, torch_transformer):\n",
    "    copy_encoder_weights(custom_transformer.encoder, torch_transformer.encoder)\n",
    "    copy_decoder_weights(custom_transformer.decoder, torch_transformer.decoder)\n",
    "def copy_attention_weights(custom_attn, torch_attn):\n",
    "    # QKV weights: PyTorch uses one big linear layer for qkv\n",
    "    qkv_weight = torch_attn.self_attn.in_proj_weight\n",
    "    qkv_bias = torch_attn.self_attn.in_proj_bias\n",
    "    d_model = qkv_weight.shape[1]\n",
    "    d_k = d_model // custom_attn.h\n",
    "\n",
    "    # Split into Q, K, V\n",
    "    custom_attn.w_q.weight.data.copy_(qkv_weight[:d_model])\n",
    "    custom_attn.w_k.weight.data.copy_(qkv_weight[d_model:2*d_model])\n",
    "    custom_attn.w_v.weight.data.copy_(qkv_weight[2*d_model:])\n",
    "    \n",
    "    # Output projection\n",
    "    custom_attn.w_o.weight.data.copy_(torch_attn.self_attn.out_proj.weight)\n",
    "\n",
    "def copy_ffn_weights(custom_ffn, torch_ffn):\n",
    "    custom_ffn.linear_1.weight.data.copy_(torch_ffn.linear1.weight)\n",
    "    custom_ffn.linear_1.bias.data.copy_(torch_ffn.linear1.bias)\n",
    "    custom_ffn.linear_2.weight.data.copy_(torch_ffn.linear2.weight)\n",
    "    custom_ffn.linear_2.bias.data.copy_(torch_ffn.linear2.bias)\n",
    "\n",
    "def copy_layer_norm(custom_ln, torch_ln):\n",
    "    custom_ln.weight.data.copy_(torch_ln.weight)\n",
    "    custom_ln.bias.data.copy_(torch_ln.bias)\n",
    "\n",
    "def copy_encoder_weights(custom_encoder, torch_encoder):\n",
    "    for i, (c_layer, t_layer) in enumerate(zip(custom_encoder.encoder_layers, torch_encoder.layers)):\n",
    "        copy_attention_weights(c_layer.multi_headed_self_attention, t_layer)\n",
    "        copy_ffn_weights(c_layer.ffn, t_layer)  # Pass layer1 for dimension reference\n",
    "        copy_layer_norm(c_layer.norm1, t_layer.norm1)\n",
    "        copy_layer_norm(c_layer.norm2, t_layer.norm2)\n",
    "\n",
    "def copy_decoder_weights(custom_decoder, torch_decoder):\n",
    "    for i, (c_layer, t_layer) in enumerate(zip(custom_decoder.decoder_layers, torch_decoder.layers)):\n",
    "        # Masked self-attention\n",
    "        qkv_weight = t_layer.self_attn.in_proj_weight\n",
    "        d_model = qkv_weight.shape[1]\n",
    "        c_layer.masked_multi_headed_self_attention.w_q.weight.data.copy_(qkv_weight[:d_model])\n",
    "        c_layer.masked_multi_headed_self_attention.w_k.weight.data.copy_(qkv_weight[d_model:2*d_model])\n",
    "        c_layer.masked_multi_headed_self_attention.w_v.weight.data.copy_(qkv_weight[2*d_model:])\n",
    "        c_layer.masked_multi_headed_self_attention.w_o.weight.data.copy_(t_layer.self_attn.out_proj.weight)\n",
    "\n",
    "        # Cross-attention\n",
    "        cross_qkv_weight = t_layer.multihead_attn.in_proj_weight\n",
    "        c_layer.multi_headed_cross_attention.w_q.weight.data.copy_(cross_qkv_weight[:d_model])\n",
    "        c_layer.multi_headed_cross_attention.w_k.weight.data.copy_(cross_qkv_weight[d_model:2*d_model])\n",
    "        c_layer.multi_headed_cross_attention.w_v.weight.data.copy_(cross_qkv_weight[2*d_model:])\n",
    "        c_layer.multi_headed_cross_attention.w_o.weight.data.copy_(t_layer.multihead_attn.out_proj.weight)\n",
    "\n",
    "        # FFN and LayerNorm\n",
    "        copy_ffn_weights(c_layer.ffn, t_layer)\n",
    "        copy_layer_norm(c_layer.norm1, t_layer.norm1)\n",
    "        copy_layer_norm(c_layer.norm2, t_layer.norm2)\n",
    "        copy_layer_norm(c_layer.norm3, t_layer.norm3)\n",
    "\n",
    "def copy_transformer_weights(custom_transformer, torch_transformer):\n",
    "    copy_encoder_weights(custom_transformer.encoder, torch_transformer.encoder)\n",
    "    copy_decoder_weights(custom_transformer.decoder, torch_transformer.decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "920cffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.transformer_tiny import Config\n",
    "from tokenizer import TokenizerWrapper\n",
    "from data import WMTENDE, CustomBatchSampler, pad_collate_fn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from model import build_transformer\n",
    "\n",
    "config = Config()\n",
    "# tokenizer = TokenizerWrapper(config)\n",
    "# data = WMTENDE(config, tokenizer, 'test')\n",
    "# data.data.shuffle(seed=20)\n",
    "# data.data = data.data.select(range(2))\n",
    "# sampler = CustomBatchSampler(len(data), batch_size=2)\n",
    "# dl = DataLoader(data, batch_sampler=sampler, collate_fn=pad_collate_fn)\n",
    "\n",
    "custom_model = build_transformer(config).transformer\n",
    "pytorch_model = torch.nn.Transformer(d_model=4, nhead=2, dropout=0.0, batch_first=True)\n",
    "# for batch in dl:\n",
    "#     src, trg = batch\n",
    "#     break\n",
    "\n",
    "src = torch.rand(1, 5, 4)\n",
    "tgt = torch.rand(1, 5, 4)\n",
    "src_mask = torch.tensor([1,1,1,0,0], dtype=bool).unsqueeze(0)\n",
    "tgt_mask = torch.tensor([1,1,1,1,0], dtype=bool).unsqueeze(0)\n",
    "copy_transformer_weights(custom_model, pytorch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e70056b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4844,  0.3405, -0.8253, -0.9997],\n",
       "         [ 1.5232,  0.2552, -0.7600, -1.0184],\n",
       "         [ 1.4834,  0.3430, -0.8295, -0.9969],\n",
       "         [ 1.4837,  0.3401, -0.8123, -1.0115],\n",
       "         [ 1.4748,  0.3561, -0.8098, -1.0210]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_model(src, tgt, src_key_padding_mask=src_mask, tgt_key_padding_mask=tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ce3e4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4844,  0.3405, -0.8253, -0.9997],\n",
       "         [ 1.5232,  0.2552, -0.7600, -1.0184],\n",
       "         [ 1.4834,  0.3430, -0.8295, -0.9969],\n",
       "         [ 1.4837,  0.3402, -0.8123, -1.0116],\n",
       "         [ 1.4748,  0.3561, -0.8098, -1.0210]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_model(src, tgt, src_mask, tgt_mask, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a171a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
