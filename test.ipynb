{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "138115e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neetm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "c:\\Users\\neetm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
    "special_token_dict = {\"bos_token\": \"<s>\"}\n",
    "tokenizer.add_special_tokens(special_token_dict)\n",
    "\n",
    "tokenizer.encode(\"hi my name is neet\")\n",
    "import torch\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class WMTDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path, src_tokenizer, tgt_tokenizer, seq_len):\n",
    "        super().__init__()\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.src_vocab_size = src_tokenizer.vocab_size\n",
    "        self.tgt_vocab_size = tgt_tokenizer.vocab_size\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sos_token = self.src_tokenizer.encode(['<s>'])[0]\n",
    "        eos_token = self.src_tokenizer.encode(['</s>'])[0]\n",
    "        pad_token = self.src_tokenizer.encode(['<pad>'])[0]\n",
    "        src_encoding = self.src_tokenizer.encode(self.data.iloc[index]['en'])[:-1] # remove default eos token\n",
    "        tgt_encoding = self.tgt_tokenizer.encode(self.data.iloc[index]['de'])[:-1] # remove default eos token\n",
    "        print(\"len of src sen: \", len(src_encoding))\n",
    "        print(\"len of tgt sen: \", len(tgt_encoding))\n",
    "        assert len(src_encoding) < self.seq_len + 2, \"sentence too big\"\n",
    "        assert len(tgt_encoding) < self.seq_len + 2, \"sentence too big\"\n",
    "        \n",
    "        src_padding_len = self.seq_len - (len(src_encoding) + 2)  \n",
    "        tgt_padding_len = self.seq_len - (len(tgt_encoding) + 2) \n",
    "        \n",
    "        src_encoding = torch.tensor([sos_token] + src_encoding + [eos_token] + [pad_token]*src_padding_len, dtype=torch.long)\n",
    "        tgt_encoding = torch.tensor([sos_token] + tgt_encoding + [eos_token] + [pad_token]*tgt_padding_len, dtype=torch.long)\n",
    "        \n",
    "        causal_mask = torch.triu(torch.ones(self.seq_len, self.seq_len, dtype=bool), diagonal=1).to(bool)\n",
    "\n",
    "        src_mask = (src_encoding == pad_token).unsqueeze(0)\n",
    "        tgt_mask = (tgt_encoding == pad_token).unsqueeze(0)\n",
    "                \n",
    "        return src_encoding, tgt_encoding, src_mask, tgt_mask\n",
    "    \n",
    "ds = WMTDataset(\"wmt14_translate_de-en_test.csv\", tokenizer, tokenizer, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e33cfba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from model import build_transformer, generate_causal_mask\n",
    "from config import TransformerConfig\n",
    "\n",
    "config = TransformerConfig()\n",
    "\n",
    "model = build_transformer(config)\n",
    "\n",
    "dataloader = DataLoader(ds, batch_size=2, shuffle=True)\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.convert_tokens_to_ids(\"<pad>\"), label_smoothing=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a355f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of src sen:  10\n",
      "len of tgt sen:  15\n",
      "len of src sen:  26\n",
      "len of tgt sen:  34\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    src, tgt, src_mask, tgt_mask = batch\n",
    "    enc_self_attn_mask = src_mask.unsqueeze(2) | src_mask.unsqueeze(3)\n",
    "    \n",
    "    causal_mask = generate_causal_mask(200)\n",
    "    \n",
    "    dec_self_attn_mask = tgt_mask.unsqueeze(2) | tgt_mask.unsqueeze(3) | causal_mask\n",
    "    \n",
    "    dec_cross_attn_mask = tgt_mask.unsqueeze(3) | src_mask.unsqueeze(2)\n",
    "    \n",
    "    y = model(src, tgt, enc_self_attn_mask, dec_self_attn_mask, dec_cross_attn_mask)\n",
    "    \n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e61124f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TransformerConfig' object has no attribute 'label_smooting'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokenizer\u001b[38;5;241m.\u001b[39mpad_token), label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smooting\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TransformerConfig' object has no attribute 'label_smooting'"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.convert_tokens_to_ids(tokenizer.pad_token), label_smoothing=config.label_smooting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1a02003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f43a31f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "# Suppose dataset is your Dataset instance\n",
    "sliced_dataset = Subset(ds, indices=range(0, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a77d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(config.train_data_path)\n",
    "\n",
    "# Slice top 2 rows\n",
    "df_overfit = df.head(2)\n",
    "\n",
    "# Save to new CSV\n",
    "df_overfit.to_csv(\"overfit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed2d222f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(15, 100)\n",
    "z = torch.randint(0, 5, (15,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f5dbe7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.7501)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(-y,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90a140df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3709, 0.0303, 0.7093,  ..., 0.3078, 0.6270, 0.5365],\n",
       "        [0.1380, 0.1782, 0.8531,  ..., 0.7949, 0.5710, 0.3463],\n",
       "        [0.3684, 0.7031, 0.5402,  ..., 0.0320, 0.9080, 0.7795],\n",
       "        ...,\n",
       "        [0.0842, 0.5291, 0.9343,  ..., 0.4297, 0.9429, 0.2803],\n",
       "        [0.7740, 0.4271, 0.4210,  ..., 0.2886, 0.3065, 0.2006],\n",
       "        [0.2793, 0.2835, 0.6560,  ..., 0.7539, 0.9754, 0.7148]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a102fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "nn.Transformer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
