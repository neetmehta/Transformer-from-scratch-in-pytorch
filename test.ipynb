{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33cfba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from model import build_transformer\n",
    "from utils import generate_causal_mask, greedy_decode, calculate_bleu_score\n",
    "from configs.transformer_tiny import Config\n",
    "from transformers import AutoTokenizer\n",
    "from data import WMTDataset\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/wmt19-en-de\")\n",
    "special_token_dict = {\"bos_token\": \"<s>\"}\n",
    "tokenizer.add_special_tokens(special_token_dict)\n",
    "\n",
    "tokenizer.encode(\"hi my name is neet\")\n",
    "    \n",
    "ds = WMTDataset(\"./wmt14_translate_de-en_train.csv\", tokenizer, tokenizer, 400)\n",
    "\n",
    "config = Config()\n",
    "\n",
    "model = build_transformer(config)\n",
    "\n",
    "\n",
    "state_dict = torch.load(\"best_ckpt_tiny.pth\")['model_state_dict']\n",
    "\n",
    "model.load_state_dict(state_dict=state_dict)\n",
    "\n",
    "dataloader = DataLoader(ds, batch_size=1, shuffle=True)\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.convert_tokens_to_ids(\"<pad>\"), label_smoothing=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a355f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "for batch in dataloader:\n",
    "    src, tgt, label, src_mask, tgt_mask = batch\n",
    "    # enc_self_attn_mask = src_mask.unsqueeze(2) | src_mask.unsqueeze(3)\n",
    "    # causal_mask = generate_causal_mask(config.seq_len)\n",
    "    # dec_self_attn_mask = tgt_mask.unsqueeze(2) | tgt_mask.unsqueeze(3) | causal_mask\n",
    "    # dec_cross_attn_mask = tgt_mask.unsqueeze(3) | src_mask.unsqueeze(2)\n",
    "    \n",
    "    # y = model(src, tgt, enc_self_attn_mask, dec_self_attn_mask, dec_cross_attn_mask)\n",
    "    print(step)\n",
    "    step +=1\n",
    "    break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61124f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, text = greedy_decode(src, src_mask, model, tokenizer, config, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78553a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(label[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6590ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(src[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c53d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c85801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.text import BLEUScore\n",
    "preds = ['Von daher werden sie gegen ihren Ex-Coach sicher ganz besonders motiviert sein.']\n",
    "target = [['Von daher werden sie gegen ihren Ex-Coach sicher ganz besonders motiviert sein.', 'Von daher werden sie gegen ihren Ex-Coach sicher ganz besonders motiviert sein.']]\n",
    "bleu = BLEUScore()\n",
    "bleu(preds, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc771797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "steps = torch.arange(1, 200_000)\n",
    "lr = (1/256**0.5)*torch.minimum(torch.pow(steps, -0.5), steps*(8000**-1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a83edd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neetm\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Reusing dataset wmt14 (C:\\Users\\neetm\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\3d7d25048da28a2f2a8dda5ca306bdc4affcf02bbcb3d277cfe2d5d7b1d71ebc)\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\neetm\\.cache\\huggingface\\datasets\\wmt14\\de-en\\1.0.0\\3d7d25048da28a2f2a8dda5ca306bdc4affcf02bbcb3d277cfe2d5d7b1d71ebc\\cache-978472542c48b526.arrow\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 'tokens': 'dict' object cannot be converted to 'PyList'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m dataset\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mrename_column(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation.en\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation_src\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m data \u001b[38;5;241m=\u001b[39m batch_iterator_src(dataset)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mtrain_bpe_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m37000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbpe.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Transformer-from-scratch-in-pytorch\\tokenizer.py:53\u001b[0m, in \u001b[0;36mtrain_bpe_tokenizer\u001b[1;34m(data, vocab_size, save)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# 6. Set post-processing template to add BOS and EOS\u001b[39;00m\n\u001b[0;32m     44\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpost_processor \u001b[38;5;241m=\u001b[39m TemplateProcessing(\n\u001b[0;32m     45\u001b[0m     single\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[BOS] $A [EOS]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     46\u001b[0m     pair\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[BOS] $A [EOS] [BOS] $B [EOS]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m     ],\n\u001b[0;32m     51\u001b[0m )\n\u001b[1;32m---> 53\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpad_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[PAD]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39msave(save)\n",
      "\u001b[1;31mTypeError\u001b[0m: argument 'tokens': 'dict' object cannot be converted to 'PyList'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizer import train_bpe_tokenizer, batch_iterator_src\n",
    "\n",
    "dataset = load_dataset(\"wmt14\", \"de-en\", split=\"test\").shuffle(seed=42)\n",
    "dataset = dataset.flatten()\n",
    "dataset=dataset.rename_column('translation.de','translation_trg')\n",
    "dataset=dataset.rename_column('translation.en','translation_src')\n",
    "\n",
    "data = batch_iterator_src(dataset)\n",
    "\n",
    "train_bpe_tokenizer(data, 37000, \"bpe.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3736d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import load_tokenizer\n",
    "\n",
    "tokenizer = load_tokenizer(\"bpe.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94f660ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = tokenizer.encode(\"hi my name is neet\", padding=\"max_length\", max_length=128, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58fb48bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,   64,   65,  997, 4843,  124,  176,  191,    3,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1c19e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
